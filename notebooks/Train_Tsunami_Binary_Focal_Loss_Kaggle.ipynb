{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd48091",
   "metadata": {},
   "source": [
    "# Binary Tsunami Detection with Focal Loss - Kaggle\n",
    "## CNN-LSTM Model Training Pipeline\n",
    "\n",
    "**Setup Instructions:**\n",
    "1. Upload this repository as a Kaggle Dataset:\n",
    "   - Go to Kaggle Datasets ‚Üí New Dataset\n",
    "   - Upload the entire repository folder\n",
    "   - Name it: `india-tsunami-early-warning`\n",
    "2. In this notebook, go to Settings ‚Üí Accelerator ‚Üí GPU T4 x2\n",
    "3. Add the dataset: Add Data ‚Üí Your Datasets ‚Üí `india-tsunami-early-warning`\n",
    "4. Run all cells\n",
    "\n",
    "**OR** Clone from GitHub directly (shown in Cell 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fe44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Setup: Clone repo from GitHub (if not using Kaggle dataset)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggle working directory\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "# Option 1: Clone from GitHub\n",
    "repo_path = '/kaggle/working/India-specific-tsunami-early-warning-system'\n",
    "if not os.path.exists(repo_path):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    !git clone https://github.com/vsiva763-git/India-specific-tsunami-early-warning-system.git\n",
    "    print(\"‚úì Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úì Repository already present\")\n",
    "    # Pull latest changes\n",
    "    !cd {repo_path} && git pull origin main\n",
    "\n",
    "# Option 2: If using Kaggle dataset (uncomment if you uploaded as dataset)\n",
    "# repo_path = '/kaggle/input/india-tsunami-early-warning'\n",
    "\n",
    "print(f\"‚úì Working directory: {os.getcwd()}\")\n",
    "print(f\"‚úì Repository path: {repo_path}\")\n",
    "\n",
    "# Verify structure\n",
    "if os.path.exists(f'{repo_path}/config/config.yaml'):\n",
    "    print(\"‚úì Config file found\")\n",
    "else:\n",
    "    print(\"‚ö† Config file missing - check repository structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749993a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Install any missing dependencies\n",
    "Kaggle has most packages pre-installed\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "# Kaggle usually has TensorFlow 2.x installed\n",
    "# Only install missing packages\n",
    "!pip install -q pyyaml loguru\n",
    "\n",
    "print(\"‚úì Dependencies ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Import libraries and verify GPU\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Add repo to path\n",
    "repo_path = '/kaggle/working/India-specific-tsunami-early-warning-system'\n",
    "# If using Kaggle dataset, uncomment:\n",
    "# repo_path = '/kaggle/input/india-tsunami-early-warning'\n",
    "\n",
    "sys.path.insert(0, repo_path)\n",
    "\n",
    "# Project imports\n",
    "from src.models.cnn_lstm_binary_model import TsunamiPredictionBinaryModel, focal_loss\n",
    "from src.models.model_trainer import ModelTrainer\n",
    "from src.models.data_preprocessor import DataPreprocessor\n",
    "\n",
    "print(\"‚úì Imports loaded\")\n",
    "print(f\"‚úì TensorFlow: {tf.__version__}\")\n",
    "print(f\"‚úì Keras: {tf.keras.__version__}\")\n",
    "\n",
    "# Check GPU\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"\\nüöÄ GPU Available: {len(gpu_devices)} device(s)\")\n",
    "    for i, gpu in enumerate(gpu_devices):\n",
    "        print(f\"   GPU {i}: {gpu.name}\")\n",
    "    # Enable memory growth to avoid OOM\n",
    "    for gpu in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"\\n‚ö† No GPU detected - training will be slower\")\n",
    "    print(\"   Enable GPU: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")\n",
    "\n",
    "# Load config\n",
    "config_path = Path(repo_path) / 'config' / 'config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(f\"\\n‚úì Config loaded: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Create balanced training data with sample weights\n",
    "40% positive class (tsunami events)\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Creating synthetic training data...\")\n",
    "\n",
    "n_samples = 10000\n",
    "n_timesteps = 24  # 24-hour temporal window\n",
    "n_features = 32   # Combined earthquake + ocean + spatial features\n",
    "\n",
    "# Features: combined all modalities\n",
    "X_data = np.random.randn(n_samples, n_timesteps, n_features).astype(np.float32)\n",
    "\n",
    "# Labels with balanced distribution (40% positive)\n",
    "y_balanced = np.random.choice([0, 1], size=n_samples, p=[0.6, 0.4]).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Add some signal: positive samples have higher amplitude\n",
    "X_data[y_balanced.flatten() == 1] *= 1.5\n",
    "\n",
    "print(f\"‚úì Data shape: {X_data.shape}\")\n",
    "print(f\"‚úì Labels shape: {y_balanced.shape}\")\n",
    "print(f\"‚úì Class distribution:\")\n",
    "unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    pct = 100 * count / len(y_balanced)\n",
    "    print(f\"   Class {int(label)}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Stratified train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_data, y_balanced,\n",
    "    test_size=0.2,\n",
    "    stratify=y_balanced,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"‚úì Val set: {X_val.shape[0]} samples\")\n",
    "print(f\"‚úì Train positive: {(y_train == 1).sum()} ({100*(y_train == 1).sum()/len(y_train):.1f}%)\")\n",
    "print(f\"‚úì Val positive: {(y_val == 1).sum()} ({100*(y_val == 1).sum()/len(y_val):.1f}%)\")\n",
    "\n",
    "# Compute sample weights (inverse class frequency)\n",
    "class_counts = np.bincount(y_train.astype(int).flatten())\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "\n",
    "# Assign sample weights based on class\n",
    "sample_weights = np.array([class_weights[int(label)] for label in y_train.flatten()])\n",
    "\n",
    "print(f\"\\n‚úì Class weights: {dict(zip(range(len(class_weights)), class_weights))}\")\n",
    "print(f\"‚úì Sample weights range: [{sample_weights.min():.3f}, {sample_weights.max():.3f}]\")\n",
    "print(f\"‚úì Sample weights mean: {sample_weights.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Build binary CNN-LSTM model with Focal Loss\n",
    "\"\"\"\n",
    "\n",
    "print(\"Building binary tsunami detection model...\")\n",
    "\n",
    "# Create model builder\n",
    "model_builder = TsunamiPredictionBinaryModel(config)\n",
    "\n",
    "# Input shape: (timesteps, features)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Build model\n",
    "model = model_builder.build_model(input_shape=input_shape)\n",
    "\n",
    "print(f\"\\n‚úì Model built successfully\")\n",
    "print(f\"‚úì Input shape: {input_shape}\")\n",
    "print(f\"‚úì Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nüìä Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80447ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Configure training parameters for Kaggle GPU\n",
    "\"\"\"\n",
    "\n",
    "# Override config for Kaggle training\n",
    "config['model']['training']['epochs'] = 30\n",
    "config['model']['training']['batch_size'] = 128  # Larger batch for GPU\n",
    "config['model']['training']['learning_rate'] = 0.001\n",
    "config['model']['training']['early_stopping_patience'] = 7\n",
    "config['model']['training']['reduce_lr_patience'] = 3\n",
    "\n",
    "print(\"Training Configuration (Kaggle GPU):\")\n",
    "print(f\"  Epochs: {config['model']['training']['epochs']}\")\n",
    "print(f\"  Batch Size: {config['model']['training']['batch_size']} (larger for GPU)\")\n",
    "print(f\"  Learning Rate: {config['model']['training']['learning_rate']}\")\n",
    "print(f\"  Early Stopping Patience: {config['model']['training']['early_stopping_patience']}\")\n",
    "print(f\"  Reduce LR Patience: {config['model']['training']['reduce_lr_patience']}\")\n",
    "print(f\"\\nKey Features:\")\n",
    "print(f\"  ‚úì Focal Loss (Œ≥=2.0, Œ±=0.25) - handles class imbalance\")\n",
    "print(f\"  ‚úì Sample Weights - weights positives higher\")\n",
    "print(f\"  ‚úì Dropout (0.3) - prevents overfitting\")\n",
    "print(f\"  ‚úì Early Stopping - prevents overfitting\")\n",
    "print(f\"  ‚úì Reduce LR on Plateau - adaptive learning rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68dd8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train binary model with Focal Loss and sample weights\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting training with Focal Loss + Sample Weights...\\n\")\n",
    "\n",
    "# Create checkpoint directory in Kaggle working directory\n",
    "checkpoint_dir = '/kaggle/working/models/checkpoints'\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f'{checkpoint_dir}/best_model.keras',\n",
    "        monitor='val_auc',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=config['model']['training']['early_stopping_patience'],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_auc',\n",
    "        factor=0.5,\n",
    "        patience=config['model']['training']['reduce_lr_patience'],\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with sample weights\n",
    "print(f\"Train samples: {X_train.shape[0]}\")\n",
    "print(f\"Val samples: {X_val.shape[0]}\")\n",
    "print(f\"Using sample weights: min={sample_weights.min():.3f}, max={sample_weights.max():.3f}\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    sample_weight=sample_weights,\n",
    "    batch_size=config['model']['training']['batch_size'],\n",
    "    epochs=config['model']['training']['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training completed!\")\n",
    "print(f\"‚úì Best epoch: {np.argmin(history.history['val_loss']) + 1}\")\n",
    "print(f\"‚úì Best val loss: {np.min(history.history['val_loss']):.4f}\")\n",
    "print(f\"‚úì Best val AUC: {np.max(history.history['val_auc']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebb10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Plot training history\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_title('Loss (Focal Loss)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[0, 1].plot(history.history['auc'], label='Train AUC')\n",
    "axes[0, 1].plot(history.history['val_auc'], label='Val AUC')\n",
    "axes[0, 1].set_title('AUC (Key Metric)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('AUC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 0].plot(history.history['recall'], label='Train Recall')\n",
    "axes[1, 0].plot(history.history['val_recall'], label='Val Recall')\n",
    "axes[1, 0].set_title('Recall (Tsunami Detection)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 1].plot(history.history['precision'], label='Train Precision')\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Val Precision')\n",
    "axes[1, 1].set_title('Precision (False Alarm Rate)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/training_history.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training history plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Validation analysis with comprehensive metrics\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred_proba = model.predict(X_val, verbose=0)\n",
    "y_val_pred = (y_val_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Key metrics\n",
    "val_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "val_acc = (y_val.flatten() == y_val_pred).mean()\n",
    "val_recall = (y_val_pred[y_val.flatten() == 1] == 1).sum() / (y_val.flatten() == 1).sum()\n",
    "val_precision = (y_val.flatten()[y_val_pred == 1] == 1).sum() / max((y_val_pred == 1).sum(), 1)\n",
    "\n",
    "print(f\"\\nüìä Metrics:\")\n",
    "print(f\"  AUC:       {val_auc:.4f} (0.5 = random, 1.0 = perfect)\")\n",
    "print(f\"  Accuracy:  {val_acc:.4f}\")\n",
    "print(f\"  Recall:    {val_recall:.4f} (% of tsunamis detected)\")\n",
    "print(f\"  Precision: {val_precision:.4f} (% of alarms correct)\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val.flatten(), y_val_pred)\n",
    "print(f\"\\nüìã Confusion Matrix:\")\n",
    "print(f\"  TN: {cm[0,0]:.0f}  FP: {cm[0,1]:.0f}\")\n",
    "print(f\"  FN: {cm[1,0]:.0f}  TP: {cm[1,1]:.0f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìà Classification Report:\")\n",
    "print(classification_report(y_val.flatten(), y_val_pred, \n",
    "                          target_names=['No Tsunami', 'Tsunami'],\n",
    "                          digits=4))\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val.flatten(), y_val_pred_proba)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC={val_auc:.4f})')\n",
    "ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax.set_title('ROC Curve (Validation Set)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/roc_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì ROC curve saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242aa6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Threshold analysis - find optimal threshold\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "thresholds_to_test = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    y_pred_th = (y_val_pred_proba > threshold).astype(int).flatten()\n",
    "    \n",
    "    tp = ((y_val.flatten() == 1) & (y_pred_th == 1)).sum()\n",
    "    fp = ((y_val.flatten() == 0) & (y_pred_th == 1)).sum()\n",
    "    fn = ((y_val.flatten() == 1) & (y_pred_th == 0)).sum()\n",
    "    tn = ((y_val.flatten() == 0) & (y_pred_th == 0)).sum()\n",
    "    \n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'accuracy': accuracy,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tn': tn\n",
    "    })\n",
    "\n",
    "df_thresh = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nüìä Threshold Performance:\")\n",
    "print(df_thresh.to_string(index=False))\n",
    "\n",
    "# Find best threshold by F1 score\n",
    "df_thresh['f1'] = 2 * (df_thresh['precision'] * df_thresh['recall']) / (df_thresh['precision'] + df_thresh['recall'] + 1e-8)\n",
    "best_threshold = df_thresh.loc[df_thresh['f1'].idxmax(), 'threshold']\n",
    "best_f1 = df_thresh.loc[df_thresh['f1'].idxmax(), 'f1']\n",
    "\n",
    "print(f\"\\n‚≠ê Best Threshold: {best_threshold} (F1={best_f1:.4f})\")\n",
    "print(f\"   At this threshold:\")\n",
    "print(f\"   - Recall: {df_thresh[df_thresh['threshold']==best_threshold]['recall'].values[0]:.4f}\")\n",
    "print(f\"   - Precision: {df_thresh[df_thresh['threshold']==best_threshold]['precision'].values[0]:.4f}\")\n",
    "\n",
    "# Plot threshold analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metrics vs threshold\n",
    "axes[0].plot(df_thresh['threshold'], df_thresh['recall'], 'o-', label='Recall', linewidth=2, markersize=8)\n",
    "axes[0].plot(df_thresh['threshold'], df_thresh['precision'], 's-', label='Precision', linewidth=2, markersize=8)\n",
    "axes[0].plot(df_thresh['threshold'], df_thresh['accuracy'], '^-', label='Accuracy', linewidth=2, markersize=8)\n",
    "axes[0].axvline(best_threshold, color='red', linestyle='--', label=f'Best (Œ∏={best_threshold})', linewidth=2)\n",
    "axes[0].set_xlabel('Threshold', fontsize=11)\n",
    "axes[0].set_ylabel('Score', fontsize=11)\n",
    "axes[0].set_title('Metrics vs Threshold', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix at best threshold\n",
    "y_pred_best = (y_val_pred_proba > best_threshold).astype(int).flatten()\n",
    "cm_best = confusion_matrix(y_val.flatten(), y_pred_best)\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', ax=axes[1], \n",
    "            xticklabels=['No Tsunami', 'Tsunami'],\n",
    "            yticklabels=['No Tsunami', 'Tsunami'])\n",
    "axes[1].set_title(f'Confusion Matrix at Œ∏={best_threshold}', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label', fontsize=11)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/threshold_analysis.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Threshold analysis plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test on hold-out test set\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create separate test set\n",
    "X_test = np.random.randn(2000, X_train.shape[1], X_train.shape[2]).astype(np.float32)\n",
    "y_test = np.random.choice([0, 1], size=2000, p=[0.6, 0.4]).astype(np.float32).reshape(-1, 1)\n",
    "X_test[y_test.flatten() == 1] *= 1.5  # Add signal\n",
    "\n",
    "print(f\"\\nTest set: {X_test.shape[0]} samples\")\n",
    "print(f\"Positive: {(y_test == 1).sum()} ({100*(y_test == 1).sum()/len(y_test):.1f}%)\")\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_test_pred = (y_test_pred_proba > best_threshold).astype(int).flatten()\n",
    "\n",
    "# Metrics\n",
    "test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "test_acc = (y_test.flatten() == y_test_pred).mean()\n",
    "test_recall = (y_test_pred[y_test.flatten() == 1] == 1).sum() / (y_test.flatten() == 1).sum()\n",
    "test_precision = (y_test.flatten()[y_test_pred == 1] == 1).sum() / max((y_test_pred == 1).sum(), 1)\n",
    "\n",
    "print(f\"\\nüìä Test Metrics (threshold={best_threshold}):\")\n",
    "print(f\"  AUC:       {test_auc:.4f}\")\n",
    "print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test.flatten(), y_test_pred)\n",
    "print(f\"\\nüìã Test Confusion Matrix:\")\n",
    "print(f\"  TN: {cm_test[0,0]:.0f}  FP: {cm_test[0,1]:.0f}\")\n",
    "print(f\"  FN: {cm_test[1,0]:.0f}  TP: {cm_test[1,1]:.0f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìà Test Classification Report:\")\n",
    "print(classification_report(y_test.flatten(), y_test_pred,\n",
    "                          target_names=['No Tsunami', 'Tsunami'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1757309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Save trained model and metadata\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create models directory in Kaggle working directory\n",
    "models_dir = Path('/kaggle/working/models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = models_dir / 'tsunami_detection_binary_focal.keras'\n",
    "model.save(str(model_path))\n",
    "print(f\"\\n‚úì Model saved: {model_path}\")\n",
    "print(f\"  File size: {model_path.stat().st_size / 1e6:.2f} MB\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'Binary CNN-LSTM with Focal Loss',\n",
    "    'input_shape': (X_train.shape[1], X_train.shape[2]),\n",
    "    'output': 'Binary classification (tsunami/no-tsunami)',\n",
    "    'threshold': best_threshold,\n",
    "    'validation_auc': float(val_auc),\n",
    "    'validation_recall': float(val_recall),\n",
    "    'validation_precision': float(val_precision),\n",
    "    'test_auc': float(test_auc),\n",
    "    'test_recall': float(test_recall),\n",
    "    'test_precision': float(test_precision),\n",
    "    'focal_loss_gamma': 2.0,\n",
    "    'focal_loss_alpha': 0.25,\n",
    "    'training_samples': int(X_train.shape[0]),\n",
    "    'positive_class_ratio': float((y_train == 1).sum() / len(y_train)),\n",
    "    'platform': 'Kaggle GPU'\n",
    "}\n",
    "\n",
    "metadata_path = models_dir / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úì Metadata saved: {metadata_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "‚úÖ Binary Model with Focal Loss Successfully Trained on Kaggle GPU!\n",
    "\n",
    "üìä Architecture:\n",
    "   - CNN blocks: 2 (32, 64 filters)\n",
    "   - LSTM layers: 2 (128, 64 units)\n",
    "   - Dense layers: 3 (128, 64, 32 units)\n",
    "   - Total parameters: {model.count_params():,}\n",
    "\n",
    "üéØ Key Features:\n",
    "   ‚úì Focal Loss (Œ≥=2.0, Œ±=0.25) - focuses on hard examples\n",
    "   ‚úì Sample Weights - upweights positive class\n",
    "   ‚úì 40% positive class in training data\n",
    "   ‚úì Stratified train/val split\n",
    "   ‚úì Early stopping and LR reduction\n",
    "\n",
    "üìà Performance (Val Set):\n",
    "   AUC: {val_auc:.4f}\n",
    "   Recall: {val_recall:.4f} (detects {100*val_recall:.1f}% of tsunamis)\n",
    "   Precision: {val_precision:.4f}\n",
    "   Best Threshold: {best_threshold}\n",
    "\n",
    "üß™ Performance (Test Set):\n",
    "   AUC: {test_auc:.4f}\n",
    "   Recall: {test_recall:.4f}\n",
    "   Precision: {test_precision:.4f}\n",
    "\n",
    "üìÅ Output Files (in /kaggle/working/):\n",
    "   - tsunami_detection_binary_focal.keras\n",
    "   - model_metadata.json\n",
    "   - training_history.png\n",
    "   - roc_curve.png\n",
    "   - threshold_analysis.png\n",
    "\n",
    "üíæ Download: Click \"Output\" tab above to download all files\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387dda0f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Download trained model**: Go to the \"Output\" tab and download:\n",
    "   - `tsunami_detection_binary_focal.keras`\n",
    "   - `model_metadata.json`\n",
    "   - All visualization plots\n",
    "\n",
    "2. **Use the model**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('tsunami_detection_binary_focal.keras')\n",
    "probability = model.predict(your_data)\n",
    "```\n",
    "\n",
    "3. **Adjust threshold**: Use the threshold analysis to pick the optimal threshold for your use case:\n",
    "   - Lower threshold (0.3-0.4): More sensitive, catches more tsunamis but more false alarms\n",
    "   - Higher threshold (0.6-0.7): More conservative, fewer false alarms but may miss some events\n",
    "\n",
    "4. **Integrate into production**: Deploy the model to your early warning system infrastructure"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
