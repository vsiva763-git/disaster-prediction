{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111434c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Setup: Ensure repo present, correct CWD, and config available\n",
    "import os, sys, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = 'https://github.com/vsiva763-git/India-specific-tsunami-early-warning-system.git'\n",
    "REPO_DIR = '/content/India-specific-tsunami-early-warning-system'\n",
    "\n",
    "# Always work from /content in Colab first\n",
    "try:\n",
    "    os.chdir('/content')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Clone if repo missing or config missing\n",
    "need_clone = (not os.path.isdir(REPO_DIR)) or (not os.path.isfile(os.path.join(REPO_DIR, 'config/config.yaml')))\n",
    "if need_clone:\n",
    "    if os.path.isdir(REPO_DIR):\n",
    "        shutil.rmtree(REPO_DIR, ignore_errors=True)\n",
    "    print('[Setup] Cloning repository...')\n",
    "    # Use shell to clone\n",
    "    import subprocess\n",
    "    subprocess.run(['git', 'clone', REPO_URL], check=True)\n",
    "\n",
    "# Enter repo and ensure import path\n",
    "os.chdir(REPO_DIR)\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "cfg_path = Path('config/config.yaml')\n",
    "print(f\"[Setup] CWD: {os.getcwd()}\")\n",
    "print(f\"[Setup] Config exists: {cfg_path.exists()} at {cfg_path}\")\n",
    "if not cfg_path.exists():\n",
    "    # Show directory contents to help debug\n",
    "    cfg_dir = Path('config')\n",
    "    print('[Setup] Listing config directory:')\n",
    "    print(list(cfg_dir.glob('*')))\n",
    "    raise FileNotFoundError('config/config.yaml not found. Please re-run the setup cell or reclone.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837c96c",
   "metadata": {},
   "source": [
    "# ðŸŒŠ Tsunami Prediction Model Training - Google Colab\n",
    "\n",
    "This notebook trains the CNN-LSTM tsunami prediction model using Google Colab's free GPU resources.\n",
    "\n",
    "**Runtime:** GPU (recommended) or TPU\n",
    "\n",
    "**Steps:**\n",
    "1. Install dependencies\n",
    "2. Clone repository or upload files\n",
    "3. Prepare training data\n",
    "4. Train the model\n",
    "5. Download trained model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f4410",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Step 1: Check GPU/TPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def936e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"\\nGPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TPU Available:\", tf.config.list_physical_devices('TPU'))\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"\\nâœ“ GPU detected - enabling mixed precision training\")\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea2c8f",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 2: Mount Google Drive (Optional)\n",
    "\n",
    "Mount Google Drive to save models and data persistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66794c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "project_dir = '/content/drive/MyDrive/tsunami_warning_system'\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/models', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data', exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Project directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf118538",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 3: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ebf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/vsiva763-git/India-specific-tsunami-early-warning-system.git\n",
    "\n",
    "# Change to project directory\n",
    "%cd India-specific-tsunami-early-warning-system\n",
    "\n",
    "# List files\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d578b",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 4: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c864e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure valid working directory (prevents Colab pip CWD error)\n",
    "import os\n",
    "repo_dir = '/content/India-specific-tsunami-early-warning-system'\n",
    "try:\n",
    "    os.chdir('/content')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Install required packages using Colab-aware %pip\n",
    "%pip install -q tensorflow==2.18.0\n",
    "%pip install -q pandas numpy scipy scikit-learn\n",
    "%pip install -q matplotlib seaborn plotly\n",
    "%pip install -q pyyaml joblib loguru\n",
    "%pip install -q xarray netCDF4\n",
    "\n",
    "# Return to repo directory if it exists\n",
    "if os.path.isdir(repo_dir):\n",
    "    os.chdir(repo_dir)\n",
    "    print(f\"âœ“ Returned to repo: {os.getcwd()}\")\n",
    "\n",
    "print(\"âœ“ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409101e2",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 5: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we're in the repo root and importable\n",
    "import os, sys\n",
    "repo_dir = '/content/India-specific-tsunami-early-warning-system'\n",
    "if os.path.isdir(repo_dir):\n",
    "    os.chdir(repo_dir)\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.insert(0, repo_dir)\n",
    "\n",
    "from src.utils.data_helpers import create_sample_dataset\n",
    "from src.utils.logger import setup_logger\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logger(log_dir='logs', level='INFO')\n",
    "\n",
    "# 1) Create synthetic training data (more samples for stability)\n",
    "print(\"Creating synthetic training dataset with balanced classes...\")\n",
    "create_sample_dataset(output_dir='data/raw', n_samples=10000)\n",
    "\n",
    "# 2) Rebalance to ~40% positive (much higher than 20% to combat class imbalance)\n",
    "raw_csv = Path('data/raw/global_tsunami_events.csv')\n",
    "df = pd.read_csv(raw_csv)\n",
    "\n",
    "pos = df[df['tsunami_occurred'] == 1]\n",
    "neg = df[df['tsunami_occurred'] == 0]\n",
    "\n",
    "if len(pos) > 0:\n",
    "    # Target: 40% positive, 60% negative\n",
    "    target_total = len(df)\n",
    "    target_pos = int(0.40 * target_total)\n",
    "    target_neg = int(0.60 * target_total)\n",
    "    \n",
    "    # Upsample/downsample as needed\n",
    "    pos_resampled = pos.sample(n=min(target_pos, len(pos) * 3), replace=True, random_state=42)\n",
    "    neg_resampled = neg.sample(n=min(target_neg, len(neg)), replace=False, random_state=42)\n",
    "    \n",
    "    df_balanced = pd.concat([pos_resampled, neg_resampled], axis=0)\n",
    "else:\n",
    "    df_balanced = df\n",
    "\n",
    "# Shuffle for unbiased train/val split\n",
    "df_balanced = df_balanced.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "df_balanced.to_csv(raw_csv, index=False)\n",
    "\n",
    "print(f\"âœ“ Balanced dataset: {(df_balanced['tsunami_occurred'].sum() / len(df_balanced) * 100):.1f}% positive\")\n",
    "print(f\"âœ“ Total samples: {len(df_balanced)}\")\n",
    "\n",
    "# 3) Prepare processed data artifacts\n",
    "from src.utils.data_helpers import prepare_training_data\n",
    "print(\"\\nProcessing training data...\")\n",
    "prepare_training_data(raw_data_dir='data/raw', processed_data_dir='data/processed')\n",
    "\n",
    "print(\"\\nâœ“ Training data ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d23565",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 6: Configure Model\n",
    "\n",
    "You can adjust training parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we're in the repo root and modules are importable\n",
    "import os, sys\n",
    "repo_dir = '/content/India-specific-tsunami-early-warning-system'\n",
    "if os.path.isdir(repo_dir):\n",
    "    os.chdir(repo_dir)\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.insert(0, repo_dir)\n",
    "\n",
    "from src.utils.config_loader import load_config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config('config/config.yaml')\n",
    "\n",
    "# Tune training parameters to reduce overfitting\n",
    "config['model']['training']['epochs'] = 30\n",
    "config['model']['training']['batch_size'] = 64\n",
    "config['model']['training']['learning_rate'] = 0.0005\n",
    "config['model']['training']['early_stopping_patience'] = 7\n",
    "config['model']['training']['reduce_lr_patience'] = 3\n",
    "\n",
    "# Increase dropout for regularization (if supported in config)\n",
    "config['model']['architecture']['dropout_rate'] = 0.5\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {config['model']['training']['epochs']}\")\n",
    "print(f\"  Batch Size: {config['model']['training']['batch_size']}\")\n",
    "print(f\"  Learning Rate: {config['model']['training']['learning_rate']}\")\n",
    "print(f\"  Early Stopping Patience: {config['model']['training']['early_stopping_patience']}\")\n",
    "print(f\"  Reduce LR Patience: {config['model']['training']['reduce_lr_patience']}\")\n",
    "print(f\"  Dropout: {config['model']['architecture']['dropout_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07c4dd",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 7: Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import TsunamiPredictionModel, DataPreprocessor\n",
    "\n",
    "# Initialize model and preprocessor\n",
    "print(\"Initializing model...\")\n",
    "model = TsunamiPredictionModel(config)\n",
    "preprocessor = DataPreprocessor(config)\n",
    "\n",
    "# Build model architecture\n",
    "print(\"\\nBuilding CNN-LSTM architecture...\")\n",
    "model.build_model(\n",
    "    earthquake_shape=(10, 4),   # 10 earthquakes, 4 features\n",
    "    ocean_shape=(5, 3),         # 5 locations, 3 features\n",
    "    spatial_shape=(64, 64, 2),  # 64x64 grid, 2 channels\n",
    "    temporal_window=72\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "print(\"\\nCompiling model...\")\n",
    "model.compile_model()\n",
    "\n",
    "print(\"\\nâœ“ Model ready for training\")\n",
    "print(\"\\nModel Summary:\")\n",
    "print(model.get_model_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97dd34",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 8: Train the Model\n",
    "\n",
    "This will take several minutes depending on the number of epochs and GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we're in the repo root and modules are importable\n",
    "import os, sys\n",
    "repo_dir = '/content/India-specific-tsunami-early-warning-system'\n",
    "if os.path.isdir(repo_dir):\n",
    "    os.chdir(repo_dir)\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.insert(0, repo_dir)\n",
    "\n",
    "from src.models import ModelTrainer\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Ensure dataset exists even if Step 5 was skipped\n",
    "raw_csv = Path('data/raw/global_tsunami_events.csv')\n",
    "if not raw_csv.exists():\n",
    "    print(\"[Setup] global_tsunami_events.csv not found. Creating and balancing now...\")\n",
    "    from src.utils.data_helpers import create_sample_dataset, prepare_training_data\n",
    "    create_sample_dataset(output_dir='data/raw', n_samples=10000)\n",
    "    df = pd.read_csv(raw_csv)\n",
    "    pos = df[df['tsunami_occurred'] == 1]\n",
    "    neg = df[df['tsunami_occurred'] == 0]\n",
    "    if len(pos) > 0:\n",
    "        target_total = len(df)\n",
    "        target_pos = int(0.40 * target_total)\n",
    "        target_neg = int(0.60 * target_total)\n",
    "        pos_resampled = pos.sample(n=min(target_pos, len(pos) * 3), replace=True, random_state=42)\n",
    "        neg_resampled = neg.sample(n=min(target_neg, len(neg)), replace=False, random_state=42)\n",
    "        df = pd.concat([pos_resampled, neg_resampled], axis=0)\n",
    "        df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "        df.to_csv(raw_csv, index=False)\n",
    "        print(\"[Setup] Synthetic dataset balanced and saved.\")\n",
    "    prepare_training_data(raw_data_dir='data/raw', processed_data_dir='data/processed')\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(config, model, preprocessor)\n",
    "\n",
    "# Prepare training data\n",
    "print(\"Loading training data...\")\n",
    "X_train, y_train, X_val, y_val = trainer.prepare_training_data('data/raw')\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train[0])}\")\n",
    "print(f\"Validation samples: {len(X_val[0])}\")\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "y_train_binary = (y_train[0].ravel() > 0.5).astype(int)\n",
    "classes = np.array([0, 1])\n",
    "class_weights_array = compute_class_weight('balanced', classes=classes, y=y_train_binary)\n",
    "class_weights = {i: float(w) for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "print(f\"\\nClass weights (to penalize minority class):\")\n",
    "print(f\"  Class 0 (no tsunami): {class_weights[0]:.3f}\")\n",
    "print(f\"  Class 1 (tsunami): {class_weights[1]:.3f}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = trainer.train(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val,\n",
    "    checkpoint_dir='models/checkpoints'\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ“ TRAINING COMPLETED in {training_time/60:.2f} minutes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa9391",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 9: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "trainer.plot_training_history()\n",
    "\n",
    "# Display final metrics\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"  Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Val Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "if 'risk_probability_accuracy' in history.history:\n",
    "    print(f\"  Risk Accuracy: {history.history['risk_probability_accuracy'][-1]:.4f}\")\n",
    "    print(f\"  Val Risk Accuracy: {history.history['val_risk_probability_accuracy'][-1]:.4f}\")\n",
    "\n",
    "if 'risk_probability_auc' in history.history:\n",
    "    print(f\"  AUC: {history.history['risk_probability_auc'][-1]:.4f}\")\n",
    "    print(f\"  Val AUC: {history.history['val_risk_probability_auc'][-1]:.4f}\")\n",
    "\n",
    "# Additional validation diagnostics\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Predict on validation set\n",
    "preds = model.model.predict(X_val, verbose=0)\n",
    "# Model outputs: [risk_probability, confidence, risk_class]\n",
    "risk_prob_pred = preds[0].ravel()\n",
    "\n",
    "# True labels for risk probability\n",
    "y_true = y_val[0].ravel()\n",
    "\n",
    "# AUC and thresholded metrics\n",
    "try:\n",
    "    val_auc = roc_auc_score(y_true, risk_prob_pred)\n",
    "    print(f\"\\n[Diagnostics] Validation ROC AUC: {val_auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[Diagnostics] Could not compute ROC AUC: {e}\")\n",
    "\n",
    "threshold = 0.5\n",
    "y_hat = (risk_prob_pred >= threshold).astype(int)\n",
    "print(\"[Diagnostics] Confusion Matrix:\\n\", confusion_matrix(y_true.astype(int), y_hat))\n",
    "print(\"[Diagnostics] Classification Report:\\n\", classification_report(y_true.astype(int), y_hat, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cddbef",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 10: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save model to local Colab storage\n",
    "model_path = 'models/best_model.keras'\n",
    "model.save_model(model_path)\n",
    "print(f\"âœ“ Model saved to: {model_path}\")\n",
    "\n",
    "# Save preprocessor scalers\n",
    "scaler_dir = 'models/scalers'\n",
    "os.makedirs(scaler_dir, exist_ok=True)\n",
    "preprocessor.save_scalers(scaler_dir)\n",
    "print(f\"âœ“ Scalers saved to: {scaler_dir}\")\n",
    "\n",
    "# Copy to Google Drive (if mounted)\n",
    "try:\n",
    "    import shutil\n",
    "    drive_model_path = '/content/drive/MyDrive/tsunami_warning_system/models/best_model.keras'\n",
    "    drive_scaler_dir = '/content/drive/MyDrive/tsunami_warning_system/models/scalers'\n",
    "    \n",
    "    shutil.copy(model_path, drive_model_path)\n",
    "    os.makedirs(drive_scaler_dir, exist_ok=True)\n",
    "    shutil.copytree(scaler_dir, drive_scaler_dir, dirs_exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nâœ“ Model also saved to Google Drive:\")\n",
    "    print(f\"  {drive_model_path}\")\n",
    "    print(f\"  {drive_scaler_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Could not save to Google Drive (not mounted or error): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f8046",
   "metadata": {},
   "source": [
    "## â¬‡ï¸ Step 11: Download Model Files\n",
    "\n",
    "Download the trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a47fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Create a zip file with model and scalers\n",
    "zip_path = 'tsunami_model.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    # Add model file\n",
    "    zipf.write('models/best_model.keras', 'best_model.keras')\n",
    "    \n",
    "    # Add scaler files\n",
    "    for root, dirs, file_list in os.walk('models/scalers'):\n",
    "        for file in file_list:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.join('scalers', file)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(\"âœ“ Created zip file with model and scalers\")\n",
    "\n",
    "# Download the zip file\n",
    "print(\"\\nDownloading... (check your browser downloads)\")\n",
    "files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7eada0",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 12: Test the Trained Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create test data\n",
    "print(\"Testing model with synthetic data...\\n\")\n",
    "\n",
    "test_eq = np.random.randn(1, 10, 4)\n",
    "test_ocean = np.random.randn(1, 5, 3)\n",
    "test_spatial = np.random.randn(1, 64, 64, 2)\n",
    "\n",
    "# Make prediction\n",
    "risk_prob, confidence, risk_class = model.predict(\n",
    "    test_eq, test_ocean, test_spatial\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Test Prediction Results:\")\n",
    "print(f\"  Risk Probability: {risk_prob[0][0]:.3f}\")\n",
    "print(f\"  Confidence: {confidence[0][0]:.3f}\")\n",
    "print(f\"  Risk Class Probabilities: {risk_class[0]}\")\n",
    "\n",
    "class_names = ['None', 'Low', 'Medium', 'High']\n",
    "predicted_class = np.argmax(risk_class[0])\n",
    "print(f\"  Predicted Risk Level: {class_names[predicted_class]}\")\n",
    "\n",
    "print(\"\\nâœ“ Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0b609",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "### After Training:\n",
    "\n",
    "1. **Download the model** (done in Step 11)\n",
    "\n",
    "2. **Deploy to your server:**\n",
    "   ```bash\n",
    "   # Extract the downloaded zip file\n",
    "   unzip tsunami_model.zip\n",
    "   \n",
    "   # Copy files to your project\n",
    "   cp best_model.keras models/\n",
    "   cp -r scalers models/\n",
    "   ```\n",
    "\n",
    "3. **Run the application:**\n",
    "   ```bash\n",
    "   python main.py\n",
    "   ```\n",
    "\n",
    "4. **Or test monitoring:**\n",
    "   ```bash\n",
    "   python monitor.py --once\n",
    "   ```\n",
    "\n",
    "### Training Tips:\n",
    "\n",
    "- **More epochs**: Set `epochs = 100` or more for better accuracy\n",
    "- **Real data**: Replace synthetic data with actual NOAA tsunami database\n",
    "- **GPU runtime**: Use GPU for faster training (Runtime â†’ Change runtime type â†’ GPU)\n",
    "- **Save checkpoints**: Best model is automatically saved during training\n",
    "- **Monitor training**: Watch the loss curves to avoid overfitting\n",
    "\n",
    "### Model Performance:\n",
    "\n",
    "Expected performance with sufficient training:\n",
    "- Validation accuracy: > 90%\n",
    "- AUC score: > 0.85\n",
    "- Inference time: < 2 seconds\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŒŠ Your tsunami prediction model is now trained and ready to use!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
